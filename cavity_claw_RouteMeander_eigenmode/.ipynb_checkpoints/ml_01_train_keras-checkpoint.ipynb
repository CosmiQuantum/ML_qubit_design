{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374b8e1e",
   "metadata": {},
   "source": [
    "# Model Training (cavity_claw_RouteMeander_eigenmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7659a-fe1b-4fdf-8c09-a398e498373b",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9418886-6a3f-4473-ae89-53bab6428eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter file is where the hyperparameters are set\n",
    "\n",
    "from parameters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d17135-58ce-45e4-9c16-1d1b76f34ea3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa89948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 04:22:17.058814: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-22 04:22:17.058897: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-22 04:22:17.058914: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 04:22:17.065417: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, LeakyReLU\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Disable some console warnings\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35ed7bf-9c4f-41d4-8652-1fe11dd8c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "# Input seed value. if this value is the same, the random number generator will generate the same set of random values every time\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the seed value for reproducibility in tensorflow\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9e65e-f247-4388-a274-6041e8cdcc27",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17dc079f-6430-41bc-8342-0cc5ffbe4a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12260521446017124441\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 8502378496\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10453252877945739739\n",
      "physical_device_desc: \"device: 0, name: NVIDIA A100 80GB PCIe MIG 1g.10gb, pci bus id: 0000:00:10.0, compute capability: 8.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 04:22:27.052438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-22 04:22:27.142112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-22 04:22:27.142903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-22 04:22:27.241064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-22 04:22:27.241658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-22 04:22:27.242165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-22 04:22:27.242684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 8108 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe MIG 1g.10gb, pci bus id: 0000:00:10.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ad03d-ae5d-4bc6-b055-3e8579849c5d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d0b8c-6699-4caf-b257-abc4f8e49b99",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667c238f-0e0f-4e4c-b185-f76d1ca261ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_AUGMENTATION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mDATA_AUGMENTATION\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTry Both\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ENCODING_TYPE:\n\u001b[1;32m      3\u001b[0m         X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/npy/x_train_augmented.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(DATA_DIR), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_AUGMENTATION' is not defined"
     ]
    }
   ],
   "source": [
    "if DATA_AUGMENTATION:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        X_train = np.load('{}/npy/x_train_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val = np.load('{}/npy/x_val_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test = np.load('{}/npy/x_test_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train = np.load('{}/npy/y_train_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val = np.load('{}/npy/y_val_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test = np.load('{}/npy/y_test_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "    \n",
    "    elif 'Try Both' in ENCODING_TYPE:\n",
    "        X_train_one_hot_encoding = np.load('{}/npy/x_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_one_hot_encoding = np.load('{}/npy/x_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_one_hot_encoding = np.load('{}/npy/x_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train_one_hot_encoding = np.load('{}/npy/y_train_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val_one_hot_encoding = np.load('{}/npy/y_val_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test_one_hot_encoding = np.load('{}/npy/y_test_one_hot_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        X_train_linear_encoding = np.load('{}/npy/x_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_linear_encoding = np.load('{}/npy/x_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_linear_encoding = np.load('{}/npy/x_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train_linear_encoding = np.load('{}/npy/y_train_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val_linear_encoding = np.load('{}/npy/y_val_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test_linear_encoding = np.load('{}/npy/y_test_linear_encoding_augmented.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "else:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        X_train = np.load('{}/npy/x_train.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val = np.load('{}/npy/x_val.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test = np.load('{}/npy/x_test.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train = np.load('{}/npy/y_train.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val = np.load('{}/npy/y_val.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test = np.load('{}/npy/y_test.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "    \n",
    "    elif 'Try Both' in ENCODING_TYPE:\n",
    "        X_train_one_hot_encoding = np.load('{}/npy/x_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_one_hot_encoding = np.load('{}/npy/x_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_one_hot_encoding = np.load('{}/npy/x_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train_one_hot_encoding = np.load('{}/npy/y_train_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val_one_hot_encoding = np.load('{}/npy/y_val_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test_one_hot_encoding = np.load('{}/npy/y_test_one_hot_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "\n",
    "        X_train_linear_encoding = np.load('{}/npy/x_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_val_linear_encoding = np.load('{}/npy/x_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        X_test_linear_encoding = np.load('{}/npy/x_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_train_linear_encoding = np.load('{}/npy/y_train_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_val_linear_encoding = np.load('{}/npy/y_val_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n",
    "        y_test_linear_encoding = np.load('{}/npy/y_test_linear_encoding.npy'.format(DATA_DIR), allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec9efd-ee93-429d-95b4-4e6efef296db",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb45684",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    print('X_train.shape: ', X_train.shape)\n",
    "    print('X_val.shape: ', X_val.shape)\n",
    "    print('y_train.shape: ', y_train.shape)\n",
    "    print('y_val.shape: ', y_val.shape)\n",
    "    print('y_train[0]: ', y_train[0])\n",
    "else:\n",
    "    print('X_train_linear_encoding.shape: ', X_train_linear_encoding.shape)\n",
    "    print('X_val_linear_encoding.shape: ', X_val_linear_encoding.shape)\n",
    "    print('y_train_linear_encoding.shape: ', y_train_linear_encoding.shape)\n",
    "    print('y_val_linear_encoding.shape: ', y_val_linear_encoding.shape)\n",
    "    print('y_train_linear_encoding[0]: ', y_train_linear_encoding[0])\n",
    "\n",
    "    print('X_train_one_hot_encoding.shape: ', X_train_one_hot_encoding.shape)\n",
    "    print('X_val_one_hot_encoding.shape: ', X_val_one_hot_encoding.shape)\n",
    "    print('y_train_one_hot_encoding.shape: ', y_train_one_hot_encoding.shape)\n",
    "    print('y_val_one_hot_encoding.shape: ', y_val_one_hot_encoding.shape)\n",
    "    print('y_train_one_hot_encoding[0]: ', y_train_one_hot_encoding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e90f9a-14a8-41e2-ad9a-cfe4e3000d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    display(X_train) #can check this in previous script as well after loading to make sure it matches\n",
    "else:\n",
    "    display(X_train_one_hot_encoding)\n",
    "    display(X_train_linear_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95debf17-6de3-49fc-9064-b01c985c3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    total = len(X_train) + len(X_test) + len(X_val)\n",
    "    print('---------------------------------------')  \n",
    "    print('Train set shape x:                {}, {:.2f}%'.format(len(X_train), (len(X_train)*100.)/total))\n",
    "    print('Validation set shape x:           {}, {:.2f}%'.format(len(X_train), (len(X_val)*100.)/total))\n",
    "    print('Test set shape x:                 {}, {:.2f}%'.format(len(X_test), (len(X_test)*100.)/total))\n",
    "    print('---------------------------------------')\n",
    "\n",
    "    total = len(y_train) + len(y_test) + len(y_val)\n",
    "    print('---------------------------------------')  \n",
    "    print('Train set shape y:                {}, {:.2f}%'.format(len(y_train), (len(y_train)*100.)/total))\n",
    "    print('Validation set shape y:           {}, {:.2f}%'.format(len(y_val), (len(y_val)*100.)/total))\n",
    "    print('Test set shape y:                 {}, {:.2f}%'.format(len(y_test), (len(y_test)*100.)/total))\n",
    "    print('---------------------------------------')\n",
    "else:\n",
    "    total = len(X_train_one_hot_encoding) + len(X_test_one_hot_encoding) + len(X_val_one_hot_encoding)\n",
    "    print('---------------------------------------')  \n",
    "    print('Train set shape x one_hot_encoding:      {}, {:.2f}%'.format(len(X_train_one_hot_encoding), (len(X_train_one_hot_encoding)*100.)/total))\n",
    "    print('Validation set shape x one_hot_encoding: {}, {:.2f}%'.format(len(X_train_one_hot_encoding), (len(X_val_one_hot_encoding)*100.)/total))\n",
    "    print('Test set shape x one_hot_encoding:       {}, {:.2f}%'.format(len(X_test_one_hot_encoding), (len(X_test_one_hot_encoding)*100.)/total))\n",
    "    print('---------------------------------------')\n",
    "\n",
    "    total = len(y_train_one_hot_encoding) + len(y_test_one_hot_encoding) + len(y_val_one_hot_encoding)\n",
    "    print('---------------------------------------')  \n",
    "    print('Train set shape y one_hot_encoding:         {}, {:.2f}%'.format(len(y_train_one_hot_encoding), (len(y_train_one_hot_encoding)*100.)/total))\n",
    "    print('Validation set shape y one_hot_encoding:    {}, {:.2f}%'.format(len(y_val_one_hot_encoding), (len(y_val_one_hot_encoding)*100.)/total))\n",
    "    print('Test set shape y one_hot_encoding:          {}, {:.2f}%'.format(len(y_test_one_hot_encoding), (len(y_test_one_hot_encoding)*100.)/total))\n",
    "\n",
    "\n",
    "    total = len(X_train_linear_encoding) + len(X_test_linear_encoding) + len(X_val_linear_encoding)\n",
    "    print('---------------------------------------')  \n",
    "    print('Train set shape x linear_encoding:      {}, {:.2f}%'.format(len(X_train_linear_encoding), (len(X_train_linear_encoding)*100.)/total))\n",
    "    print('Validation set shape x linear_encoding: {}, {:.2f}%'.format(len(X_train_linear_encoding), (len(X_val_linear_encoding)*100.)/total))\n",
    "    print('Test set shape x linear_encoding:       {}, {:.2f}%'.format(len(X_test_linear_encoding), (len(X_test_linear_encoding)*100.)/total))\n",
    "    print('---------------------------------------')\n",
    "\n",
    "    total = len(y_train_linear_encoding) + len(y_test_linear_encoding) + len(y_val_linear_encoding)\n",
    "    print('---------------------------------------')  \n",
    "    print('Train set shape y linear_encoding:         {}, {:.2f}%'.format(len(y_train_linear_encoding), (len(y_train_linear_encoding)*100.)/total))\n",
    "    print('Validation set shape y linear_encoding:    {}, {:.2f}%'.format(len(y_val_linear_encoding), (len(y_val_linear_encoding)*100.)/total))\n",
    "    print('Test set shape y linear_encoding:          {}, {:.2f}%'.format(len(y_test_linear_encoding), (len(y_test_linear_encoding)*100.)/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b32e05-b5df-4647-8dae-6ee5f8cddcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    #--------------------Training Set---------------------\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    \n",
    "    # Histogram for Cavity Frequency\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([i[0] for i in X_train], bins=30, color='lightblue', edgecolor='black')\n",
    "    plt.title('Training Set Histogram of Cavity Frequency')\n",
    "    plt.xlabel('Cavity Frequency (Hz)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Histogram for Kappa\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist([i[1] for i in X_train], bins=30, color='salmon', edgecolor='black')\n",
    "    plt.title('Training Set Histogram of Kappa')\n",
    "    plt.xlabel('Kappa')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #--------------------Validation Set---------------------\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    \n",
    "    # Histogram for Cavity Frequency\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([i[0] for i in X_val], bins=30, color='lightblue', edgecolor='black')\n",
    "    plt.title('Validation Set Histogram of Cavity Frequency')\n",
    "    plt.xlabel('Cavity Frequency (Hz)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Histogram for Kappa\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist([i[1] for i in X_val], bins=30, color='salmon', edgecolor='black')\n",
    "    plt.title('Validation Set Histogram of Kappa')\n",
    "    plt.xlabel('Kappa')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #--------------------Test Set---------------------\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    \n",
    "    # Histogram for Cavity Frequency\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([i[0] for i in X_test], bins=30, color='lightblue', edgecolor='black')\n",
    "    plt.title('Test Set Histogram of Cavity Frequency')\n",
    "    plt.xlabel('Cavity Frequency (Hz)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Histogram for Kappa\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist([i[1] for i in X_test], bins=30, color='salmon', edgecolor='black')\n",
    "    plt.title('Test Set Histogram of Kappa')\n",
    "    plt.xlabel('Kappa')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    #--------------------Training Set---------------------\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    \n",
    "    # Histogram for Cavity Frequency\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([i[0] for i in X_train_linear_encoding], bins=30, color='lightblue', edgecolor='black')\n",
    "    plt.title('Training Set Histogram of Cavity Frequency')\n",
    "    plt.xlabel('Cavity Frequency (Hz)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Histogram for Kappa\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist([i[1] for i in X_train_linear_encoding], bins=30, color='salmon', edgecolor='black')\n",
    "    plt.title('Training Set Histogram of Kappa')\n",
    "    plt.xlabel('Kappa')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #--------------------Validation Set---------------------\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    \n",
    "    # Histogram for Cavity Frequency\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([i[0] for i in X_val_linear_encoding], bins=30, color='lightblue', edgecolor='black')\n",
    "    plt.title('Validation Set Histogram of Cavity Frequency')\n",
    "    plt.xlabel('Cavity Frequency (Hz)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Histogram for Kappa\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist([i[1] for i in X_val_linear_encoding], bins=30, color='salmon', edgecolor='black')\n",
    "    plt.title('Validation Set Histogram of Kappa')\n",
    "    plt.xlabel('Kappa')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #--------------------Test Set---------------------\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    \n",
    "    # Histogram for Cavity Frequency\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([i[0] for i in X_test_linear_encoding], bins=30, color='lightblue', edgecolor='black')\n",
    "    plt.title('Test Set Histogram of Cavity Frequency')\n",
    "    plt.xlabel('Cavity Frequency (Hz)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Histogram for Kappa\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist([i[1] for i in X_test_linear_encoding], bins=30, color='salmon', edgecolor='black')\n",
    "    plt.title('Test Set Histogram of Kappa')\n",
    "    plt.xlabel('Kappa')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecc581",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cf19c",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4f8c5",
   "metadata": {},
   "source": [
    "Create a classical multi-layer perceptron for regression: 602 input features, 10 targets, and a certain number of hidden layers and neurons. Taking some inspiration from [Deep learning-based I-V Global Parameter Extraction for BSIM-CMG](https://www.sciencedirect.com/science/article/pii/S003811012300179X), Solid-State Electronics, Vol. 209, November 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945bd67-9923-4f7c-8e32-56672a6c3a4a",
   "metadata": {},
   "source": [
    "Reccomended to download a third party app like \"Sleep control Center\" or \"Amphetamine\" to prevent computer from sleeping during the many hour/day long training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4f485-e3da-4663-832c-3bc917660e45",
   "metadata": {},
   "source": [
    "### Create Model by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce671ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER:\n",
    "    # n output neurons for n parameters\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        # Multilayer perceptron (MLP) with 2 input features. MLP is having neurons that adjust rules based on how accurate they can guess things\n",
    "        model_shape = f'mlp_{len(X_test[0])}_'\n",
    "        \n",
    "        # Define the number on neurons in the inner layer (in parameter file)\n",
    "        model_shape += '_'.join(str(l) for l in NEURONS_PER_LAYER)\n",
    "    \n",
    "        print(len(y_train[0]))\n",
    "        model_shape += f'_{len(y_train[0])}'\n",
    "    else:\n",
    "        # Multilayer perceptron (MLP) with 2 input features. MLP is having neurons that adjust rules based on how accurate they can guess things\n",
    "        model_shape_one_hot_encoding = f'mlp_{len(X_test_one_hot_encoding[0])}_'\n",
    "        model_shape_linear_encoding = f'mlp_{len(X_test_one_hot_encoding[0])}_'\n",
    "        \n",
    "        # Define the number on neurons in the inner layer (in parameter file)\n",
    "        model_shape_one_hot_encoding += '_'.join(str(l) for l in NEURONS_PER_LAYER)\n",
    "        model_shape_linear_encoding += '_'.join(str(l) for l in NEURONS_PER_LAYER)\n",
    "    \n",
    "        print('one hot: ',len(y_train_one_hot_encoding[0]))\n",
    "        model_shape_one_hot_encoding += f'_{len(y_train_one_hot_encoding[0])}'\n",
    "        print('linear: ',len(y_train_linear_encoding[0]))\n",
    "        model_shape_linear_encoding += f'_{len(y_train_linear_encoding[0])}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba881d-a212-402c-86d8-ed5ab2bed357",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        # Initialize a sequential model, which lets us build a linear stack of layers\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Add an input layer to the model. This layer defines the shape of the input data (2 features per sample), sets dimensions of following layers\n",
    "        model.add(Input(shape=(len(X_test[0])), name='input1'))\n",
    "        \n",
    "        # Iterate over the configuration of neurons for each hidden layer specified in NEURONS_PER_LAYER\n",
    "        for i, n in enumerate(NEURONS_PER_LAYER):\n",
    "            # Add a fully connected (dense) hidden layer with spec ified number of neurons\n",
    "            # The LeCun uniform initializer is used when initializing weights, this makes the model more stable\n",
    "            # L2 regularization is used in each layer to penalizing large weights, which prevents overfitting\n",
    "            model.add(Dense(n, name='fc{}'.format(i), kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        \n",
    "            # Apply a Leaky ReLU activation function to the outputs of the dense layer\n",
    "            # This introduces non-linearities, allowing the network to learn complex functions\n",
    "            # Leaky ReLU is chosen over standard ReLU to help mitigate the \"dying ReLU\" problem:\n",
    "            #     - This problem is when neurons using the ReLU activation function output zero for all inputs and stop learning\n",
    "            #     - Can be mitigated by using variations like Leaky ReLU or proper initialization\n",
    "            model.add(LeakyReLU(alpha=0.01, name='leaky_relu{}'.format(i)))\n",
    "            \n",
    "            # Add a dropout layer to reduce overfitting -- randomly drops a set fraction (like 30%) of outputs from the layer\n",
    "            model.add(Dropout(rate=TRAIN_DROPOUT_RATE, name='dropout{}'.format(i)))\n",
    "        \n",
    "        # Add the output layer consisting of # neurons, corresponding to the # target variables we aim to predict.\n",
    "        # The same LeCun uniform initializer is used to ensure consistency and stability at the output layer as well.\n",
    "        model.add(Dense(len(y_train[0]), activation='linear', name='fc_output', kernel_initializer='lecun_uniform'))\n",
    "    \n",
    "    else:\n",
    "        model_one_hot_encoding = Sequential()\n",
    "        model_one_hot_encoding.add(Input(shape=(len(X_test_one_hot_encoding[0])), name='input1'))\n",
    "        for i, n in enumerate(NEURONS_PER_LAYER):\n",
    "            model_one_hot_encoding.add(Dense(n, name='fc{}'.format(i), kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "            model_one_hot_encoding.add(LeakyReLU(alpha=0.01, name='leaky_relu{}'.format(i)))\n",
    "            model_one_hot_encoding.add(Dropout(rate=TRAIN_DROPOUT_RATE, name='dropout{}'.format(i)))\n",
    "        model_one_hot_encoding.add(Dense(len(y_train_one_hot_encoding[0]), activation='linear', name='fc_output', kernel_initializer='lecun_uniform'))\n",
    "    \n",
    "        model_linear_encoding = Sequential()\n",
    "        model_linear_encoding.add(Input(shape=(len(X_test_linear_encoding[0])), name='input1'))\n",
    "        for i, n in enumerate(NEURONS_PER_LAYER):\n",
    "            model_linear_encoding.add(Dense(n, name='fc{}'.format(i), kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "            model_linear_encoding.add(LeakyReLU(alpha=0.01, name='leaky_relu{}'.format(i)))\n",
    "            model_linear_encoding.add(Dropout(rate=TRAIN_DROPOUT_RATE, name='dropout{}'.format(i)))\n",
    "        model_linear_encoding.add(Dense(len(y_train_linear_encoding[0]), activation='linear', name='fc_output', kernel_initializer='lecun_uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0181fd2-e4ee-4e41-aaae-4603c1a853cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER:\n",
    "    # The exponential decay learning rate schedule gradually reduces the learning rate, fine-tuning the learning process for better convergence\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=LR_INITIAL,  \n",
    "        decay_steps=LR_DECAY_STEPS,        \n",
    "        decay_rate=LR_DECAY_RATE,          \n",
    "        staircase=LR_STAIRCASE             \n",
    "    )\n",
    "    \n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        # Set model to minimize loss specified by TRAIN_LOSS, and also to report the loss during training\n",
    "        model.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),  \n",
    "            loss=TRAIN_LOSS,                                         \n",
    "            metrics=[TRAIN_LOSS]                                     \n",
    "        )\n",
    "    else:\n",
    "        # Set model to minimize loss specified by TRAIN_LOSS, and also to report the loss during training\n",
    "        model_linear_encoding.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),  \n",
    "            loss=TRAIN_LOSS,                                         \n",
    "            metrics=[TRAIN_LOSS]                                     \n",
    "        )\n",
    "        model_one_hot_encoding.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),  \n",
    "            loss=TRAIN_LOSS,                                         \n",
    "            metrics=[TRAIN_LOSS]                                     \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3657d-d0ac-40e6-9829-694436270bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER:\n",
    "    !mkdir -p model\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        best_model_file = 'model/{}_best_model.h5'.format(model_shape)\n",
    "        last_model_file = 'model/{}_last_model.h5'.format(model_shape)\n",
    "    else:\n",
    "        best_model_file_one_hot_encoding = 'model/{}_best_model_one_hot_encoding.h5'.format(model_shape_one_hot_encoding)\n",
    "        last_model_file_one_hot_encoding = 'model/{}_last_model_one_hot_encoding.h5'.format(model_shape_one_hot_encoding)\n",
    "    \n",
    "        best_model_file_linear_encoding = 'model/{}_best_model_linear_encoding.h5'.format(model_shape_linear_encoding)\n",
    "        last_model_file_linear_encoding = 'model/{}_last_model_linear_encoding.h5'.format(model_shape_linear_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49516260-5078-4162-ab99-cdb45f9f9827",
   "metadata": {},
   "source": [
    "Enable training (`train_and_save`) to overwrite the model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e4501-3aea-4053-9788-30b4532d7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2957910-f200-4e9f-91c3-6e7ed0926ceb",
   "metadata": {},
   "source": [
    "We use Adam optimizer, minimize the Mean Squared Logarithmic Error, and early stop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a00680-7b7f-4877-babb-481f0682b7b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "if not KERAS_TUNER:\n",
    "    class TrainingPlot(tf.keras.callbacks.Callback):\n",
    "         \n",
    "        # This function is called when the training begins\n",
    "        def on_train_begin(self, logs={}):\n",
    "            # Initialize the lists for holding the logs, losses \n",
    "            self.losses = []\n",
    "            self.val_losses = []\n",
    "            self.logs = []\n",
    "        \n",
    "        # This function is called at the end of each epoch\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            \n",
    "            # Append the logs, losses  to the lists\n",
    "            self.logs.append(logs)\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            \n",
    "            # Before plotting ensure at least 2 epochs have passed\n",
    "            if len(self.losses) > 1:\n",
    "                \n",
    "                # Clear the previous plot\n",
    "                clear_output(wait=True)\n",
    "                N = np.arange(0, len(self.losses))\n",
    "                \n",
    "                # Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "                plt.figure()\n",
    "                plt.plot(N, self.losses, label = \"train_loss\")\n",
    "                plt.plot(N, self.val_losses, label = \"val_loss\")\n",
    "                plt.title(\"Training Loss [Epoch {}]\".format(epoch))\n",
    "                plt.xlabel(\"Epoch #\")\n",
    "                plt.ylabel(\"Loss/Accuracy\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "           \n",
    "    class LearningRateMonitor(tf.keras.callbacks.Callback):\n",
    "        def __init__(self):\n",
    "            super(LearningRateMonitor, self).__init__()\n",
    "            self.learning_rates = []\n",
    "    \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            lr = self.model.optimizer._current_learning_rate.read_value()\n",
    "            self.learning_rates.append(lr)\n",
    "            print(f\"Epoch: {epoch + 1}, Learning Rate: {lr:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef223c28-cc33-4721-b049-be7023eb13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = None  \n",
    "if not KERAS_TUNER:\n",
    "    if train_and_save: \n",
    "        # Set up early stopping to prevent overfitting by halting training when validation loss stops improving\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',                      # Monitor validation loss for stopping criteria \n",
    "            mode='min',                              # Stop when the monitored quantity has stopped decreasing\n",
    "            patience=TRAIN_EARLY_STOPPING_PATIENCE,  # Number of epochs to wait after last improvement\n",
    "            verbose=1                                # Enable logging when early stopping happens\n",
    "        )\n",
    "    \n",
    "        # Train the model on the training data and validate on a portion of it\n",
    "        if 'Try Both' not in ENCODING_TYPE:\n",
    "            plot_callback = TrainingPlot()      # Plot training progress\n",
    "            lr_monitor = LearningRateMonitor()  # Watch learning rate changes\n",
    "            \n",
    "            # Set up model checkpointing to save the model at its best validation loss:\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=best_model_file,          \n",
    "                monitor='val_loss',            # Save the model based on validation loss improvement\n",
    "                mode='min',                    # Favor lower validation loss values for saving (minimize)\n",
    "                save_best_only=True,           # Save only when validation loss improves\n",
    "                verbose=0                      # No logging for model saving\n",
    "            )\n",
    "    \n",
    "            history = model.fit(\n",
    "                np.asarray(X_train),  \n",
    "                np.asarray(y_train),      \n",
    "                epochs=100,                   \n",
    "                batch_size=TRAIN_BATCH_SIZE,  \n",
    "                validation_data=(np.asarray(X_val), np.asarray(y_val)),  \n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback, lr_monitor],  \n",
    "                verbose=1  # Enable logging of the training process.\n",
    "            )\n",
    "            \n",
    "            model.save(last_model_file)  # Save the final model when done training!\n",
    "        \n",
    "        else:\n",
    "            #-----------------------------------------linear--------------------------------------------\n",
    "            plot_callback_linear_encoding = TrainingPlot()      # Plot training progress\n",
    "            lr_monitor_linear_encoding = LearningRateMonitor()  # Watch learning rate changes\n",
    "            \n",
    "            # Set up model checkpointing to save the model at its best validation loss:\n",
    "            model_checkpoint_linear_encoding = ModelCheckpoint(\n",
    "                filepath=best_model_file_linear_encoding,          \n",
    "                monitor='val_loss',            # Save the model based on validation loss improvement\n",
    "                mode='min',                    # Favor lower validation loss values for saving (minimize)\n",
    "                save_best_only=True,           # Save only when validation loss improves\n",
    "                verbose=0                      # No logging for model saving\n",
    "            )\n",
    "            \n",
    "            history_linear_encoding = model_linear_encoding.fit(\n",
    "                np.asarray(X_train_linear_encoding),  \n",
    "                np.asarray(y_train_linear_encoding),      \n",
    "                epochs=100,                   \n",
    "                batch_size=TRAIN_BATCH_SIZE,  \n",
    "                validation_data=(np.asarray(X_val_linear_encoding), np.asarray(y_val_linear_encoding)), \n",
    "                callbacks=[early_stopping, model_checkpoint_linear_encoding, plot_callback_linear_encoding, lr_monitor_linear_encoding],  \n",
    "                verbose=1  # Enable logging of the training process.\n",
    "            )\n",
    "            \n",
    "            model_linear_encoding.save(last_model_file_linear_encoding)  # Save the final model when done training!\n",
    "            \n",
    "            #-----------------------------------------one hot--------------------------------------------\n",
    "            plot_callback_one_hot_encoding = TrainingPlot()      # Plot training progress\n",
    "            lr_monitor_one_hot_encoding = LearningRateMonitor()  # Watch learning rate changes\n",
    "            \n",
    "            # Set up model checkpointing to save the model at its best validation loss:\n",
    "            model_checkpoint_one_hot_encoding = ModelCheckpoint(\n",
    "                filepath=best_model_file_one_hot_encoding,          \n",
    "                monitor='val_loss',            # Save the model based on validation loss improvement\n",
    "                mode='min',                    # Favor lower validation loss values for saving (minimize)\n",
    "                save_best_only=True,           # Save only when validation loss improves\n",
    "                verbose=0                      # No logging for model saving\n",
    "            )\n",
    "            \n",
    "            history_one_hot_encoding = model_one_hot_encoding.fit(\n",
    "                np.asarray(X_train_one_hot_encoding),  \n",
    "                np.asarray(y_train_one_hot_encoding),      \n",
    "                epochs=100,                   \n",
    "                batch_size=TRAIN_BATCH_SIZE,  \n",
    "                validation_data=(np.asarray(X_val_one_hot_encoding), np.asarray(y_val_one_hot_encoding)), \n",
    "                callbacks=[early_stopping, model_checkpoint_one_hot_encoding, plot_callback_one_hot_encoding, lr_monitor_one_hot_encoding],  \n",
    "                verbose=1  # Enable logging of the training process.\n",
    "            )\n",
    "            \n",
    "            model_one_hot_encoding.save(last_model_file_one_hot_encoding)  # Save the final model when done training!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4074c-a8ef-4a69-a8cd-5714edb90166",
   "metadata": {},
   "source": [
    "Load the saved best model and use it from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447145e-91b5-4070-aefe-38f8e6bc33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        model = load_model(best_model_file, custom_objects={})\n",
    "    else:\n",
    "        model_one_hot_encoding = load_model(best_model_file_one_hot_encoding, custom_objects={})\n",
    "        model_linear_encoding = load_model(best_model_file_linear_encoding, custom_objects={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002f4a5-df05-4ff2-be6d-24f8027ca4ea",
   "metadata": {},
   "source": [
    "### Keras Tuner to Find Best Hyperparameters and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe2de3-ba0a-4103-bfd1-466d73bd9b8c",
   "metadata": {},
   "source": [
    "Run this if you want to use keras tuner to make the model rather than doing it by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d804553-ae82-4f8f-9182-806c7f22b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from keras_tuner import HyperModel, RandomSearch\n",
    "    from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081aa70b-c54e-48af-b895-5e9ddd2746cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        def build_hypermodel(hp):\n",
    "            # Hyperparameters to tune\n",
    "            neurons_per_layer = [hp.Int(f'neurons_{i}', min_value=100, max_value=5000, step=100) for i in range(4)]\n",
    "            dropout_rate = hp.Float('dropout_rate', TRAIN_DROPOUT_RATE, 0.5, step=0.1)\n",
    "            \n",
    "            # Create Model in the same way that we do by hand\n",
    "            model = Sequential()\n",
    "            model.add(Input(shape=(len(X_test[0]),), name='input1'))\n",
    "        \n",
    "            for i, n in enumerate(neurons_per_layer):\n",
    "                model.add(Dense(n, name=f'fc{i}', kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "                model.add(LeakyReLU(alpha=0.01, name=f'leaky_relu{i}'))\n",
    "                model.add(Dropout(rate=dropout_rate, name=f'dropout{i}'))\n",
    "        \n",
    "            model.add(Dense(len(y_train[0]), name='output', kernel_initializer='lecun_uniform'))\n",
    "        \n",
    "            # Learning rate configuration\n",
    "            lr_initial = hp.Float('learning_rate', 1e-6, 5e-3, sampling='LOG', default=0.0001)\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=lr_initial,\n",
    "                decay_steps=LR_DECAY_STEPS,\n",
    "                decay_rate=LR_DECAY_RATE,\n",
    "                staircase=LR_STAIRCASE\n",
    "            )\n",
    "        \n",
    "            model.compile(optimizer=tf.optimizers.Adam(learning_rate=lr_schedule), \n",
    "                          loss='mean_squared_error',\n",
    "                          metrics=['mean_squared_error'])\n",
    "            return model\n",
    "    else:\n",
    "        def build_hypermodel_one_hot_encoding(hp):\n",
    "             # Hyperparameters to tune\n",
    "            neurons_per_layer = [hp.Int(f'neurons_{i}', min_value=100, max_value=5000, step=100) for i in range(4)]\n",
    "            dropout_rate = hp.Float('dropout_rate', TRAIN_DROPOUT_RATE, 0.5, step=0.1)\n",
    "            \n",
    "            #----------------------------------------------one hot-------------------------------------------\n",
    "            # Create Model in the same way that we do by hand\n",
    "            model_one_hot_encoding = Sequential()\n",
    "            model_one_hot_encoding.add(Input(shape=(len(X_test_one_hot_encoding[0]),), name='input1'))\n",
    "        \n",
    "            for i, n in enumerate(neurons_per_layer):\n",
    "                model_one_hot_encoding.add(Dense(n, name=f'fc{i}', kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "                model_one_hot_encoding.add(LeakyReLU(alpha=0.01, name=f'leaky_relu{i}'))\n",
    "                model_one_hot_encoding.add(Dropout(rate=dropout_rate, name=f'dropout{i}'))\n",
    "        \n",
    "            model_one_hot_encoding.add(Dense(len(y_train_one_hot_encoding[0]), name='output', kernel_initializer='lecun_uniform'))\n",
    "            \n",
    "            # Learning rate configuration\n",
    "            lr_initial = hp.Float('learning_rate', 1e-6, 5e-3, sampling='LOG', default=0.0001)\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=lr_initial,\n",
    "                decay_steps=LR_DECAY_STEPS,\n",
    "                decay_rate=LR_DECAY_RATE,\n",
    "                staircase=LR_STAIRCASE\n",
    "            )\n",
    "\n",
    "            optimizer = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "            model_one_hot_encoding.compile(optimizer=optimizer, \n",
    "                          loss='mean_squared_error',\n",
    "                          metrics=['mean_squared_error'])\n",
    "            return model_one_hot_encoding\n",
    "\n",
    "        def build_hypermodel_linear_encoding(hp):\n",
    "            # Hyperparameters to tune\n",
    "            neurons_per_layer = [hp.Int(f'neurons_{i}', min_value=100, max_value=5000, step=100) for i in range(4)]\n",
    "            dropout_rate = hp.Float('dropout_rate', TRAIN_DROPOUT_RATE, 0.5, step=0.1)\n",
    "            \n",
    "            #----------------------------------------------linear------------------------------------------- \n",
    "            # Create Model in the same way that we do by hand\n",
    "            model_linear_encoding = Sequential()\n",
    "            model_linear_encoding.add(Input(shape=(len(X_test_linear_encoding[0]),), name='input1'))\n",
    "        \n",
    "            for i, n in enumerate(neurons_per_layer):\n",
    "                model_linear_encoding.add(Dense(n, name=f'fc{i}', kernel_initializer='lecun_uniform', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "                model_linear_encoding.add(LeakyReLU(alpha=0.01, name=f'leaky_relu{i}'))\n",
    "                model_linear_encoding.add(Dropout(rate=dropout_rate, name=f'dropout{i}'))\n",
    "        \n",
    "            model_linear_encoding.add(Dense(len(y_train_linear_encoding[0]), name='output', kernel_initializer='lecun_uniform'))\n",
    "            #----------------------------------------------continue-------------------------------------------\n",
    "\n",
    "            # Learning rate configuration\n",
    "            lr_initial = hp.Float('learning_rate', 1e-6, 5e-3, sampling='LOG', default=0.0001)\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=lr_initial,\n",
    "                decay_steps=LR_DECAY_STEPS,\n",
    "                decay_rate=LR_DECAY_RATE,\n",
    "                staircase=LR_STAIRCASE\n",
    "            )\n",
    "            optimizer = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "            model_linear_encoding.compile(optimizer=optimizer, \n",
    "                          loss='mean_squared_error',\n",
    "                          metrics=['mean_squared_error'])\n",
    "            return model_linear_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfea157-b0b4-4148-8d0a-e37c877877a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    if KERAS_TUNER:\n",
    "        # Start tuning\n",
    "        tuner = RandomSearch(\n",
    "            build_hypermodel,\n",
    "            objective='val_loss',\n",
    "            max_trials=KERAS_TUNER_TRIALS,\n",
    "            executions_per_trial=1,\n",
    "            directory=KERAS_DIR +'/hyper_tuning',\n",
    "            project_name='mlp_tuning'\n",
    "        )\n",
    "else:\n",
    "    if KERAS_TUNER:\n",
    "        # Start tuning linear encoding\n",
    "        tuner_linear_encoding = RandomSearch(\n",
    "            build_hypermodel_linear_encoding,\n",
    "            objective='val_loss',\n",
    "            max_trials=KERAS_TUNER_TRIALS,\n",
    "            executions_per_trial=1,\n",
    "            directory=KERAS_DIR +'/hyper_tuning_linear_encoding',\n",
    "            project_name='mlp_tuning_linear_encoding'\n",
    "        )\n",
    "\n",
    "        # Start tuning one hot encoding\n",
    "        tuner_one_hot_encoding = RandomSearch(\n",
    "            build_hypermodel_one_hot_encoding,\n",
    "            objective='val_loss',\n",
    "            max_trials=KERAS_TUNER_TRIALS,\n",
    "            executions_per_trial=1,\n",
    "            directory=KERAS_DIR + '/hyper_tuning_one_hot_encoding',\n",
    "            project_name='mlp_tuning_one_hot_encoding'\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11608cb-5b05-45af-af22-0bcab2827734",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    # Setup Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        patience=TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15440869-6be6-4005-9db4-3b37695adc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        # Perform hyperparameter tuning\n",
    "        tuner.search(np.asarray(X_train), \n",
    "                     np.asarray(y_train), \n",
    "                     epochs=50, \n",
    "                     batch_size=TRAIN_BATCH_SIZE, \n",
    "                     validation_data=(np.asarray(X_val), np.asarray(y_val)),\n",
    "                     callbacks=[early_stopping], \n",
    "                     verbose=1)\n",
    "    else:\n",
    "        # Perform hyperparameter tuning\n",
    "        tuner_one_hot_encoding.search(np.asarray(X_train_one_hot_encoding), \n",
    "                     np.asarray(y_train_one_hot_encoding), \n",
    "                     epochs=50, \n",
    "                     batch_size=TRAIN_BATCH_SIZE, \n",
    "                     validation_data=(np.asarray(X_val_one_hot_encoding), np.asarray(y_val_one_hot_encoding)),\n",
    "                     callbacks=[early_stopping], \n",
    "                     verbose=1)\n",
    "\n",
    "        tuner_linear_encoding.search(np.asarray(X_train_linear_encoding), \n",
    "                     np.asarray(y_train_linear_encoding), \n",
    "                     epochs=50, \n",
    "                     batch_size=TRAIN_BATCH_SIZE, \n",
    "                     validation_data=(np.asarray(X_val_linear_encoding), np.asarray(y_val_linear_encoding)),\n",
    "                     callbacks=[early_stopping], \n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e8802-97f4-42e5-8571-36b78d2f079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        model = tuner.get_best_models(num_models=1)[0]\n",
    "        print(model)\n",
    "    else:\n",
    "        model_linear_encoding = tuner_linear_encoding.get_best_models(num_models=1)[0]\n",
    "        print(model_linear_encoding)\n",
    "        model_one_hot_encoding = tuner_one_hot_encoding.get_best_models(num_models=1)[0]\n",
    "        print(model_one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b1b58-8413-4573-a02d-fe8363165edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        best_model_file = 'model/best_keras_model.h5'\n",
    "        model.save(best_model_file)\n",
    "        \n",
    "        model = load_model(best_model_file)\n",
    "    else:\n",
    "        best_model_file_linear_encoding = 'model/best_keras_model_linear_encoding.h5'\n",
    "        best_model_file_one_hot_encoding = 'model/best_keras_model_one_hot_encoding.h5'\n",
    "        model_one_hot_encoding.save(best_model_file_one_hot_encoding)\n",
    "        model_linear_encoding.save(best_model_file_linear_encoding)\n",
    "        \n",
    "        model_linear_encoding = load_model(best_model_file_linear_encoding)\n",
    "        model_one_hot_encoding = load_model(best_model_file_one_hot_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291953b-7ce8-498d-88ce-20974516af77",
   "metadata": {},
   "source": [
    "### View the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d2a50-a4ef-4ccc-8cb9-04e380f0fcfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        model.summary()\n",
    "    else:\n",
    "        model_one_hot_encoding.summary()\n",
    "        model_linear_encoding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff4e35-6bb8-401a-9cae-a799b3bbc388",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KERAS_TUNER:\n",
    "    #pip install --upgrade pydot\n",
    "    def get_model_shape_string(model):\n",
    "        \"\"\"Extracts model shape information directly from model layers.\n",
    "    \n",
    "        Args:\n",
    "          model: A Keras model instance.\n",
    "    \n",
    "        Returns:\n",
    "          A string representing the model shape. Returns an error message if the \n",
    "          model architecture is not supported.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_shape = model.input_shape[1] # Assumes a single input layer\n",
    "            output_shape = model.output_shape[-1] # Assumes last dimension is output size\n",
    "    \n",
    "            layer_sizes = [layer.units for layer in model.layers if hasattr(layer, 'units')]\n",
    "    \n",
    "            model_shape = f\"mlp_{input_shape}_\" + \"_\".join(map(str, layer_sizes)) + f\"_{output_shape}\"\n",
    "            return model_shape\n",
    "        except (AttributeError, IndexError) as e:\n",
    "            return f\"Error getting model shape: {e}.  Unsupported model architecture?\"\n",
    "    \n",
    "\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        model_shape = get_model_shape_string(model)\n",
    "        tf.keras.utils.plot_model(model, to_file='{}_model.png'.format(model_shape), show_shapes=True, show_layer_names=True)\n",
    "    else:\n",
    "        model_shape_one_hot = get_model_shape_string(model_one_hot_encoding)\n",
    "        model_shape_linear = get_model_shape_string(model_linear_encoding)\n",
    "        tf.keras.utils.plot_model(model_linear_encoding, to_file='{}_model_linear_encoding.png'.format(model_shape_linear), show_shapes=True, show_layer_names=True)\n",
    "        tf.keras.utils.plot_model(model_one_hot_encoding, to_file='{}_model_one_hot_encoding.png'.format(model_shape_one_hot), show_shapes=True, show_layer_names=True)\n",
    "else:\n",
    "    if 'Try Both' not in ENCODING_TYPE:\n",
    "        tf.keras.utils.plot_model(model, to_file='{}_model.png'.format(model_shape), show_shapes=True, show_layer_names=True)\n",
    "    else:\n",
    "        tf.keras.utils.plot_model(model_linear_encoding, to_file='{}_model_linear_encoding.png'.format(model_shape_linear_encoding), show_shapes=True, show_layer_names=True)\n",
    "        tf.keras.utils.plot_model(model_one_hot_encoding, to_file='{}_model_one_hot_encoding.png'.format(model_shape_one_hot_encoding), show_shapes=True, show_layer_names=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f05d0a25-2890-4ddc-b888-57b849fa2a0f",
   "metadata": {},
   "source": [
    "if KERAS_TUNER:\n",
    "    keras2ascii(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57cb40-ca2c-4ac7-905c-e13beaf71a51",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5556f8-c81b-4522-b7be-d847b6f20452",
   "metadata": {},
   "source": [
    "Although we may plot and print many metrics, we focus only on **Mean Squared Error (MSE).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc9acc-ad33-4df3-ac56-8ed3913ce16d",
   "metadata": {},
   "source": [
    "Plot training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523cc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib ipympl\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347263a8-570e-4a04-bbae-3ea0662e4012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.plot(history_linear_encoding.history['loss'])\n",
    "    plt.plot(history_linear_encoding.history['val_loss'])\n",
    "    plt.title('Model loss linear encoding')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history_one_hot_encoding.history['loss'])\n",
    "    plt.plot(history_one_hot_encoding.history['val_loss'])\n",
    "    plt.title('Model loss one hot encoding')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47150b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    plt.plot(lr_monitor.learning_rates)\n",
    "    plt.title(\"Learning Rate over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.plot(lr_monitor_linear_encoding.learning_rates)\n",
    "    plt.title(\"Learning Rate over Epochs linear encoding\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(lr_monitor_one_hot_encoding.learning_rates)\n",
    "    plt.title(\"Learning Rate over Epochs one hot encoding\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a331d8-70ea-4ea4-8f5b-256472850784",
   "metadata": {},
   "source": [
    "Measure and print metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff7d16-5709-41d6-b3f7-e6c9a062b35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    test_loss_result = model.evaluate(np.asarray(X_test), np.asarray(y_test))[0]\n",
    "    print(y_test[0])\n",
    "    print('test_loss_result: ', test_loss_result)\n",
    "else:\n",
    "    test_loss_result_linear_encoding = model_linear_encoding.evaluate(np.asarray(X_test_linear_encoding), np.asarray(y_test_linear_encoding))[0]\n",
    "    test_loss_result_one_hot_encoding = model_one_hot_encoding.evaluate(np.asarray(X_test_one_hot_encoding), np.asarray(y_test_one_hot_encoding))[0]\n",
    "\n",
    "    print(y_test_linear_encoding[0])\n",
    "    print(y_test_linear_encoding[0])\n",
    "    \n",
    "    print('Current loss linear encoding {}: {}'.format(TRAIN_LOSS, test_loss_result_linear_encoding))\n",
    "    print('Current loss one hot encoding {}: {}'.format(TRAIN_LOSS, test_loss_result_one_hot_encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7812af-2596-44d2-bb25-68741e0506ba",
   "metadata": {},
   "source": [
    "## Compare predictions vs. test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60157266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    csv_data = [[\n",
    "        DATA_AUGMENTATION,\n",
    "        model_shape,\n",
    "        ENCODING_TYPE,\n",
    "        test_loss_result,\n",
    "        TRAIN_LOSS,\n",
    "        TRAIN_DROPOUT_RATE,\n",
    "        TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        '0.15/0.15',\n",
    "        LR_INITIAL,\n",
    "        LR_DECAY_STEPS,\n",
    "        LR_DECAY_RATE,\n",
    "        LR_STAIRCASE\n",
    "        ]]\n",
    "    \n",
    "    csv_file = 'history_losses.csv'  #this doesnt reqrite this file so you need to delete this if you want something fresh\n",
    "    \n",
    "    if not os.path.exists(csv_file):\n",
    "        with open(csv_file, 'w') as file:\n",
    "            file.write('data_augmentation,model_shape,encoding_type,test_loss,train_loss,train_dropout_rate,train_early_stop_patience,'+\n",
    "                        'train_batch_size,train_val_split,lr_initial,lr_decay_step,lr_decay_rate,lr_stair_case\\n')\n",
    "    \n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data)\n",
    "    \n",
    "    # Convert data to DataFrame for easier display\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    def color_red_column(s):\n",
    "        return ['color: red' if v else '' for v in s]\n",
    "    \n",
    "    styled_df = df.style.apply(color_red_column, subset=['test_loss'])\n",
    "    \n",
    "    # Display the DataFrame as a table\n",
    "    display(styled_df)\n",
    "    #qgrid_widget = qgrid.show_grid(df, show_toolbar=True)\n",
    "\n",
    "else:\n",
    "    #---------------------------------------------------one hot---------------------------------------\n",
    "    csv_data = [[\n",
    "        DATA_AUGMENTATION,\n",
    "        model_shape_one_hot_encoding,\n",
    "        'One Hot',\n",
    "        test_loss_result_one_hot_encoding,\n",
    "        TRAIN_LOSS,\n",
    "        TRAIN_DROPOUT_RATE,\n",
    "        TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        '0.15/0.15',\n",
    "        LR_INITIAL,\n",
    "        LR_DECAY_STEPS,\n",
    "        LR_DECAY_RATE,\n",
    "        LR_STAIRCASE\n",
    "        ]]\n",
    "    \n",
    "    csv_file = 'history_losses.csv'  #this doesnt reqrite this file so you need to delete this if you want something fresh\n",
    "    \n",
    "    if not os.path.exists(csv_file):\n",
    "        with open(csv_file, 'w') as file:\n",
    "            file.write('data_augmentation,model_shape,encoding_type,test_loss,train_loss,train_dropout_rate,train_early_stop_patience,'+\n",
    "                        'train_batch_size,train_val_split,lr_initial,lr_decay_step,lr_decay_rate,lr_stair_case\\n')\n",
    "            \n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data)\n",
    "    \n",
    "    # Convert data to DataFrame for easier display\n",
    "    df_one_hot_encoding = pd.read_csv(csv_file)\n",
    "    \n",
    "    def color_red_column(s):\n",
    "        return ['color: red' if v else '' for v in s]\n",
    "    \n",
    "    styled_df_one_hot_encoding = df_one_hot_encoding.style.apply(color_red_column, subset=['test_loss'])\n",
    "    #---------------------------------------------------linear---------------------------------------\n",
    "    csv_data_linear_encoding = [[\n",
    "        DATA_AUGMENTATION,\n",
    "        model_shape_linear_encoding,\n",
    "        'Linear',\n",
    "        test_loss_result_linear_encoding,\n",
    "        TRAIN_LOSS,\n",
    "        TRAIN_DROPOUT_RATE,\n",
    "        TRAIN_EARLY_STOPPING_PATIENCE,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        '0.15/0.15',\n",
    "        LR_INITIAL,\n",
    "        LR_DECAY_STEPS,\n",
    "        LR_DECAY_RATE,\n",
    "        LR_STAIRCASE\n",
    "        ]]\n",
    "    \n",
    "    csv_file_linear_encoding = 'history_losses.csv'  #this doesnt reqrite this file so you need to delete this if you want something fresh\n",
    "    \n",
    "    with open(csv_file_linear_encoding, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data_linear_encoding)\n",
    "    \n",
    "    # Convert data to DataFrame for easier display\n",
    "    df_linear_encoding = pd.read_csv(csv_file_linear_encoding)\n",
    "    \n",
    "    def color_red_column(s):\n",
    "        return ['color: red' if v else '' for v in s]\n",
    "    \n",
    "    styled_df_linear_encoding = df_linear_encoding.style.apply(color_red_column, subset=['test_loss'])\n",
    "    \n",
    "    # Display the DataFrame as a table\n",
    "    display(styled_df_linear_encoding)\n",
    "    #qgrid_widget = qgrid.show_grid(df, show_toolbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea88851-ed24-42d4-b36c-e79175af6847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#chose the one model you want to see more in depth\n",
    "if 'Try Both' not in ENCODING_TYPE:\n",
    "    y_pred = model.predict(np.array(X_test))\n",
    "    y_encoding_format_name = 'one_hot'\n",
    "else:\n",
    "    if test_loss_result_linear_encoding < test_loss_result_one_hot_encoding:\n",
    "        y_pred = model_linear_encoding.predict(np.array(X_test_linear_encoding))\n",
    "        y_test = y_test_linear_encoding\n",
    "        y_encoding_format_name = 'linear'\n",
    "    else:\n",
    "        y_pred = model_one_hot_encoding.predict(np.array(X_test_one_hot_encoding))\n",
    "        y_test = y_test_one_hot_encoding\n",
    "        y_encoding_format_name = 'one_hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ffa127-c6a4-4a52-8a29-4c08ae0cca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'y_characteristics_{y_encoding_format_name}_encoding.csv'\n",
    "with open(filename, 'r') as f:\n",
    "            headers_string = f.readline().strip()  # Read the first line (headers) and remove whitespace\n",
    "            headers = headers_string.split(',') \n",
    "#print(headers)\n",
    "\n",
    "pow2_errors = [(y_test[i]-y_pred[i][0])**2 for i in range(len(y_pred))]\n",
    "abs_errors = [abs(y_test[i]-y_pred[i][0]) for i in range(len(y_pred))]\n",
    "\n",
    "print('---------------------------------------------------Scaled predictions----------------------------------------------')\n",
    "min=1\n",
    "for i in range(2): \n",
    "    if abs_errors[i].any()<min: \n",
    "        min = abs_errors[i]\n",
    "    print(f\"---------------------- X Values: cavity_frequency:{X_test[i][0]} \tkappa: {X_test[i][1]}-----------------------\")\n",
    "    for l in range(len(y_test[i])):\n",
    "        print('Param: {}, Ref: {}, Pred: {}, Pow2Er: {}, AbsEr: {}'.format(headers[l],y_test[i][l], y_pred[i][l], pow2_errors[i][l], abs_errors[i][l]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304c67f-9838-491f-869c-e51d86748217",
   "metadata": {},
   "source": [
    "### Unscaled test vs predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05f774-e63a-4325-86eb-c091a1c286d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unscale X\n",
    "import joblib\n",
    "X_index_names = ['cavity_frequency','kappa']\n",
    "\n",
    "X_test_unscaled = X_test.copy()\n",
    "for i, row in enumerate(X_test_unscaled):\n",
    "    for j, val in enumerate(row):\n",
    "        scaler = joblib.load(f'scalers/scaler_X_{X_index_names[j]}.save')\n",
    "        X_test_unscaled[i][j] = scaler.inverse_transform([[val]])[0][0] # Access the single value from the array\n",
    "\n",
    "# X_unscaled is now a list of lists with inverse transformed values\n",
    "print(X_test_unscaled[0])\n",
    "\n",
    "\n",
    "# Unscale y\n",
    "y_test_unscaled = y_test.copy()\n",
    "for i, row in enumerate(y_test_unscaled):\n",
    "    for j, val in enumerate(row):\n",
    "        scaler = joblib.load(f'scalers/scaler_y_{headers[j]}_{y_encoding_format_name}_encoding.save')\n",
    "        y_test_unscaled[i][j] = scaler.inverse_transform([[val]])[0][0] # Access the single value from the array\n",
    "\n",
    "# X_unscaled is now a list of lists with inverse transformed values\n",
    "print(y_test_unscaled[0])\n",
    "\n",
    "# Unscale y predictions\n",
    "y_pred_unscaled = y_pred.copy()\n",
    "for i, row in enumerate(y_pred_unscaled):\n",
    "    for j, val in enumerate(row):\n",
    "        scaler = joblib.load(f'scalers/scaler_y_{headers[j]}_{y_encoding_format_name}_encoding.save')\n",
    "        y_pred_unscaled[i][j] = scaler.inverse_transform([[val]])[0][0] # Access the single value from the array\n",
    "\n",
    "# X_unscaled is now a list of lists with inverse transformed values\n",
    "print(y_pred_unscaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907324d-19c6-4111-a79a-2637bf3fa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'----------------------------------------------Unscaled predictions {y_encoding_format_name} encoding----------------------------------------')\n",
    "min=1\n",
    "for i in range(2): \n",
    "    if abs_errors[i].any()<min: \n",
    "        min = abs_errors[i]\n",
    "    print(f\"---------------------- X Values: cavity_frequency:{X_test_unscaled[i][0]} \tkappa: {X_test_unscaled[i][1]}-----------------------\")\n",
    "    for l in range(len(y_test[i])):\n",
    "        print('Param: {}, Ref: {}, Pred: {}, Pow2Er: {}, AbsEr: {}'.format(headers[l],y_test_unscaled[i][l], y_pred_unscaled[i][l], pow2_errors[i][l], abs_errors[i][l]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec484e-c04b-4570-9f2b-76f013a459a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c325d-c74d-49ab-995c-67d44e713dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276b019-12ff-48d1-bd6d-978d0b139981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079351ea-0bcb-426d-a1d7-440b4a777627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac167c-9ffb-4ae7-b732-918457d4d30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
